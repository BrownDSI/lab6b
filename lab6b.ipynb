{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 6b: Multiple models & Ensemble Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "- Introduction\n",
    " - Voting Methods \n",
    "- Ensemble Learning \n",
    "    - Bagging\n",
    "        - Random Forest\n",
    "    - Boosting\n",
    "        - AdaBoost\n",
    "        - GradientBoost\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In most areas, having multiple experts work on a problem often leads to a better solution.  The same idea can be applied to Machine Learning,  where the experts are different _estimators_ (e.g., classifiers, regressors, neural nets).  As in the real world, if the estimators \"have different perspectives\", their combination will have improved generalizability and robustness over any single one of them.\n",
    "\n",
    "Run the code below to see an example of combining the results of a three different classifiers. \n",
    "Each classifier is trained on two features of the Iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Loading some example data\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, [0, 2]]\n",
    "y = iris.target\n",
    "\n",
    "# Training classifiers\n",
    "clf1 = DecisionTreeClassifier(max_depth=4)\n",
    "clf2 = KNeighborsClassifier(n_neighbors=10)\n",
    "clf3 = SVC(kernel='rbf', probability=True)\n",
    "eclf = VotingClassifier(estimators=[('dt', clf1), ('knn', clf2),\n",
    "                                    ('svc', clf3)],\n",
    "                        voting=\"hard\", weights=[2, 1, 1])\n",
    "\n",
    "clf1.fit(X, y)\n",
    "clf2.fit(X, y)\n",
    "clf3.fit(X, y)\n",
    "eclf.fit(X, y)\n",
    "\n",
    "# Plotting decision regions\n",
    "def plot_clf(plt, clf, X, y):\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
    "                     np.arange(y_min, y_max, 0.1))\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, alpha=0.4)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=20, edgecolor='k')\n",
    "\n",
    "f, axarr = plt.subplots(2, 2, sharex='col', sharey='row', figsize=(10, 8))\n",
    "for idx, clf, tt in zip(product([0, 1], [0, 1]),\n",
    "                        [clf1, clf2, clf3, eclf],\n",
    "                        ['Decision Tree (depth=4)', 'KNN (k=7)',\n",
    "                         'Kernel SVM', 'Combination']):\n",
    "    plot_clf(axarr[idx[0], idx[1]], clf,X,y)\n",
    "    axarr[idx[0], idx[1]].set_title(tt)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Estimators / Voting\n",
    "In the above example, *voting* to used to determine the output of the combined classifier.\n",
    "\n",
    "In majority *voting*, the predicted class label for a particular sample is the class label that represents the majority (mode) of the labels predicted by each individual classifier.\n",
    "E.g., if the predictions for a given sample are\n",
    "* classifier 1 -> label A\n",
    "* classifier 2 -> label A\n",
    "* classifier 3 -> label B  \n",
    "the combined prediction is “label A”.\n",
    "\n",
    "Weights can also be applied to make some classifiers votes count more than others. \n",
    "\n",
    "Many classifiers, when given a set of features, also generate estimates of the probability of each possible label being present.  These can be probabilities can be combined to form more nuanced predictions.  \n",
    "\n",
    "One approach to this is via an approach called _soft voting_. The idea is to return the class label as argmax of the sum of predicted probabilities. Here again, weights can be be assigned to each classifier in order to favor more accurate ones (i.e. weight their estimates more heavily in the final outcome).\n",
    "\n",
    "To illustrate this with a simple example, let’s assume we have 3 classifiers and a 3-class classification problem with class labels A,B,C, using weights $w_1$, $w_2$, and $w_3$. If for a given sample, the following class label probabilities were found:\n",
    "\n",
    "\n",
    " classifier i | P_i(A) | $P_i(B) | P_i(C) \n",
    " -------------|:------------:|:------------:|:-----------:  \n",
    " classifier 1 |  0.2  | 0.5 | 0.3\n",
    " classifier 2 |  0.6  | 0.3 | 0.1\n",
    " classifier 3 |  0.3  | 0.4 | 0\n",
    "  \n",
    "\n",
    "The corresponding soft vote, S, for class k is given by\n",
    "$S(k) = w_1 P_1(k)+w_2 P_2(k) + w_3 P_3(k)$. If we assign equal weights of 1 to all classifiers, in the example above\n",
    "\n",
    " | S(A) | S(B) | S(C)\n",
    "-|------|------|----- \n",
    "S(k) |1.1 | 1.2 | 0.4\n",
    "\n",
    "Here, the predicted class label is B, since it has the highest soft vote.\n",
    "\n",
    "## Task: Hard and Soft Voting\n",
    "1. In general, when using soft max voting with n classifiers, if one uses a uniform weighting of 1/n, do the soft voting scores define a probability measure on possible class labels?\n",
    "\n",
    "2. What label would be selected as the output for each classifier in the table above?  \n",
    "Using a majority voting, what class label would be selected using these outputs?\n",
    "\n",
    "3. The [sklearn.ensemble.VotingClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html) \n",
    "allows one to perform both majority (hard) voting and soft vote class with a collection of individual classifiers.\n",
    "The `voting` parameter selects the method to use. Re-run the code above by changing the `voting` parameter from `\"hard\"` to `\"soft\"`.  \n",
    "Does it look like the combined classifier results improve, stay the same or get worse? Explain your reasoning.\n",
    "\n",
    "Put your answers below."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# YOUR ANSWERS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a [sklearn label probability example](http://scikit-learn.org/stable/auto_examples/ensemble/plot_voting_probas.html#sphx-glr-auto-examples-ensemble-plot-voting-probas-py);  It shows the label probabilities from with three different base classifiers and a voting classifier for one data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "clf1 = LogisticRegression(random_state=123)\n",
    "clf2 = RandomForestClassifier(random_state=123)\n",
    "clf3 = GaussianNB()\n",
    "X = np.array([[-1.0, -1.0], [-1.2, -1.4], [-3.4, -2.2], [1.1, 1.2]])\n",
    "y = np.array([1, 1, 2, 2])\n",
    "\n",
    "eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n",
    "                        voting='soft',\n",
    "                        weights=[2, 1, 1])\n",
    "\n",
    "# predict class probabilities for all classifiers\n",
    "probas = [c.fit(X, y).predict_proba(X) for c in (clf1, clf2, clf3, eclf)]\n",
    "\n",
    "# get class probabilities for the first sample in the dataset\n",
    "class1_1 = [pr[0, 0] for pr in probas]\n",
    "class2_1 = [pr[0, 1] for pr in probas]\n",
    "\n",
    "\n",
    "# plotting\n",
    "\n",
    "N = 4  # number of groups\n",
    "ind = np.arange(N)  # group positions\n",
    "width = 0.35  # bar width\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# bars for classifier 1-3\n",
    "p1 = ax.bar(ind, np.hstack(([class1_1[:-1], [0]])), width,\n",
    "            color='green', edgecolor='k')\n",
    "p2 = ax.bar(ind + width, np.hstack(([class2_1[:-1], [0]])), width,\n",
    "            color='lightgreen', edgecolor='k')\n",
    "\n",
    "# bars for VotingClassifier\n",
    "p3 = ax.bar(ind, [0, 0, 0, class1_1[-1]], width,\n",
    "            color='blue', edgecolor='k')\n",
    "p4 = ax.bar(ind + width, [0, 0, 0, class2_1[-1]], width,\n",
    "            color='steelblue', edgecolor='k')\n",
    "\n",
    "# plot annotations\n",
    "plt.axvline(2.8, color='k', linestyle='dashed')\n",
    "ax.set_xticks(ind + width)\n",
    "ax.set_xticklabels(['LogisticRegression\\nweight 1',\n",
    "                    'GaussianNB\\nweight 1',\n",
    "                    'RandomForestClassifier\\nweight 5',\n",
    "                    'VotingClassifier\\n(average probabilities)'],\n",
    "                   rotation=40,\n",
    "                   ha='right')\n",
    "plt.ylim([0, 1])\n",
    "plt.title('Class probabilities for sample 1 by different classifiers')\n",
    "plt.legend([p1[0], p2[0]], ['class 1', 'class 2'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameter optimization \n",
    "Independently optimizing each estimator before combining it with others is a good first step, but given it's their combined performance that is of interest, simultaneously optimizing all their hyper-parameters may yield additional improvements.  \n",
    "\n",
    "Below is an simultaneous optimization using on sklearn.GridSearchCV, in the example, voting strategies are also explored.\n",
    "\n",
    "More detailed examples involving pipeline optimization, including feature selection can be found in this [MLxtend] article](http://rasbt.github.io/mlxtend/user_guide/classifier/EnsembleVoteClassifier/) on Sebastian Raschka's `EnsembleVoteClassifier` (which became the basis for the `scikit-learn.VotingClassifier`).  See the pipeline and feature documentation at sklearn as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "clf1 = DecisionTreeClassifier()\n",
    "clf2 = KNeighborsClassifier()\n",
    "clf3 = SVC(kernel='rbf', probability=True)\n",
    "eclf = VotingClassifier(estimators=[('dt', clf1), ('knn', clf2), ('svc', clf3)])\n",
    "\n",
    "params = {'dt__max_depth': [3, 4, 5],\n",
    "          'knn__n_neighbors': [6, 7, 8],\n",
    "          'voting': ['hard','soft']}\n",
    "          \n",
    "grid = GridSearchCV(estimator=eclf, param_grid=params, cv=5, scoring = 'accuracy')\n",
    "\n",
    "X = iris.data[:, [0, 2]]\n",
    "y = iris.target\n",
    "grid.fit(X, y)\n",
    "\n",
    "def report_results(grid):\n",
    "    cv_keys = ('mean_test_score', 'std_test_score', 'params')\n",
    "    print('mean_test_score +/ std_test_score, {params}') \n",
    "    for r, _ in enumerate(grid.cv_results_['mean_test_score']):\n",
    "        bf = '*' if grid.cv_results_[cv_keys[0]][r]==grid.best_score_ else ' '\n",
    "        print(bf+\"%0.3f +/- %0.2f %r\"\n",
    "          % (grid.cv_results_[cv_keys[0]][r],\n",
    "             grid.cv_results_[cv_keys[1]][r] / 2.0,\n",
    "             grid.cv_results_[cv_keys[2]][r]))\n",
    "\n",
    "report_results(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: Working with GridSearchCV\n",
    "Using the code block below, determine the best model found by `GridSearchCV` and then do the following:\n",
    "1. Print out the best model's average accuracy score on out of fold data. \n",
    "2. Print out the best model's parameters.\n",
    "3. Use `plot_clf` to display its decision boundaries.\n",
    "\n",
    "hint: This information is available in `grid` variable that was computed in the previous cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Methods\n",
    "The goal of **ensemble methods** is to _automatically create_ the set of \n",
    "base estimators (using the method's learning algorithm) which are then combined \n",
    "using a voting approach.  \n",
    "\n",
    "Two families of ensemble methods are usually distinguished:\n",
    "- In **averaging methods**, the driving principle is to build several estimators \n",
    "independently and then to average their predictions. On average, the combined \n",
    "estimator is usually better than any of the single base estimator because \n",
    "its variance is reduced. **Examples:** \n",
    "  - [Bagging methods](http://scikit-learn.org/stable/modules/ensemble.html#bagging), \n",
    "  - [Forests of randomized trees](http://scikit-learn.org/stable/modules/ensemble.html#forest)\n",
    "\n",
    "- By contrast, in **boosting methods**, base estimators are built sequentially \n",
    "and in order to try to reduce the bias of previously constructed estimators.\n",
    "Here again, the motivation is to combine several weak models to produce a powerful ensemble. **Examples:** \n",
    "  - [AdaBoost](http://scikit-learn.org/stable/modules/ensemble.html#adaboost)\n",
    "  - [Gradient Tree Boosting](http://scikit-learn.org/stable/modules/ensemble.html#gradient-boosting), … "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging\n",
    "\n",
    "Bagging is a powerful method to improve the performance of simple models and reduce overfitting of more complex models. With bagging, several models are fitted on different samples (with replacement) of the population. Then, these models are aggregated by using their average, weighted average or a voting system.\n",
    "\n",
    "A key insight for bagging is that by averaging (or generally aggregating) many low bias, high variance models, we can reduce the variance while retaining the low bias. Here’s an example of this for density estimation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://qph.ec.quoracdn.net/main-qimg-55c44d63831742ddd387541a428fcedf.webp\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each estimate is centered around the true density, but is overly complicated (low bias, high variance). By averaging them out, we get a smoothed version of them (low variance), still centered around the true density (low bias). (Jonathan Gordon on Quora)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forests\n",
    "\n",
    "A random forest is a variant of bagging which results in a more random but potentially more powerful classifier. While training our individual decision trees, we randomly select a subset of features to choose our best splits from. This makes the base decision trees more different from one another.\n",
    "\n",
    "To summaryize, additional random variable selection is introduced in random forests to make the underlying trees even more independent, which makes it perform better than ordinary bagging.\n",
    "\n",
    "Here is an example of a random forest in sklearn converted into a standard bagging classifier without randomized best splits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier \n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=20, max_features=None) # no randomized splitting when max_features=None\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting \n",
    "\n",
    "Boosting is a general ensemble method that creates a stronger model from a number of weaker models.\n",
    "\n",
    "This is done by building a model from the training data, then creating a second model that attempts to correct the errors from the first model. Models are added until the training set is predicted perfectly or a maximum number of models are added."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adaptive Boosting (AdaBoost)\n",
    "\n",
    "The core principle of AdaBoost is to fit a sequence of weak learners (i.e., models that are only slightly better than random guessing, such as small decision trees) on weighted versions of the data. As iterations proceed, examples that are difficult to predict receive ever-increasing influence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/boosting1.png\" style=\"width:600px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) We start with one [decision tree stump](https://en.wikipedia.org/wiki/Decision_stump) to classify training samples.  \n",
    "(2) In the next round, we then train another decision tree stump that focuses on getting the samples that were misclassified in (1). We achieve this by putting a larger weight on the previously misclassified training samples.  \n",
    "(3) The 2nd classifier will likely get some other samples wrong, so we would re-adjust the weights and train the third classifier accordingly.  \n",
    "(4) Same logic from (3) is applied.\n",
    "\n",
    "\n",
    "In a nutshell, we can summarize “Adaboost” as “adaptive” or “incremental” learning from mistakes. Eventually, we will come up with a model that has a lower bias than an individual decision tree (thus, it is less likely to underfit the training data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting\n",
    "\n",
    "Gradient Boosting is just another popular boosting technique that is very similar to Adaptive Boosting.  \n",
    "The major difference is that Gradient Boosting identifies the ‘shortcomings’ of weaker learners by gradients in the loss function instead of the alpha weights performance in Adaptive Boosting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scikit-learn Ensemble Methods\n",
    "You will now use the sci-kit to setup and run ensemble method optimizations using the techniques introduced earlier. Understanding the useful model parameters associated with a these powerful classification approaches takes time and practice, and as with all classification approaches, what works well will be problem domain or even data set specific.  \n",
    "\n",
    "Please run the code below to create and display a blob data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from itertools import cycle\n",
    "from sklearn.datasets import make_blobs\n",
    "X, y = make_blobs(n_samples=1000, centers=20, random_state=42)\n",
    "y = y % 2\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=20, edgecolor='k'); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: Ensemble training\n",
    "Read the hyper-linked documentation and build models on the  blob dataset (X, y) using the following approaches:\n",
    "\n",
    "(1) RandomForestClassifier  \n",
    "(2) BaggingClassifier  \n",
    "(3) AdaBoostClassifier  \n",
    "(4) GradientBoostingClassifier  \n",
    "\n",
    "Below is a fully worked example for building a *DecisionTreeClassifier* on the blob dataset.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** [DecisionTreeClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) **  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier \n",
    "\n",
    "# Step 1) Display an example\n",
    "clf = DecisionTreeClassifier(max_depth=4)\n",
    "\n",
    "clf.fit(X, y)\n",
    "plot_clf(plt, clf, X, y)\n",
    "plt.show()\n",
    "\n",
    "# Step 2) Pick parameters to optimize\n",
    "params = {'max_depth': [2, 4, 8, 16], 'min_samples_leaf':[2, 4, 8]}\n",
    "\n",
    "DTgrid = GridSearchCV(estimator=clf, param_grid=params, cv=5, scoring = 'accuracy')\n",
    "DTgrid.fit(X,y)\n",
    "plot_clf(plt, DTgrid, X, y); plt.show()\n",
    "report_results(DTgrid) # Show Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** [RandomForestClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) **  \n",
    "List of hyperparameters to tune: (max_features, n_estimators, min_sample_leaf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier \n",
    "\n",
    "# Step 1) Display an example\n",
    "# YOUR CODE HERE\n",
    "\n",
    "clf.fit(X, y)\n",
    "plot_clf(plt, clf, X, y); plt.show()\n",
    "\n",
    "# Step 2) Pick parameters to optimize\n",
    "# YOUR CODE HERE\n",
    "\n",
    "RFgrid = GridSearchCV(estimator=clf, param_grid=params, cv=5, scoring = 'accuracy')\n",
    "RFgrid.fit(X,y)\n",
    "plot_clf(plt, RFgrid, X, y); plt.show()\n",
    "report_results(RFgrid) # Show Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** [BaggingClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html) **  \n",
    "List of hyperparameters to tune: (base_estimator, max_features, n_estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "# Step 1) Display an example\n",
    "# YOUR CODE HERE\n",
    "\n",
    "clf.fit(X, y)\n",
    "plot_clf(plt, clf, X, y); plt.show()\n",
    "\n",
    "# Step 2) Pick parameters to optimize\n",
    "# YOUR CODE HERE\n",
    "\n",
    "BCgrid = GridSearchCV(estimator=clf, param_grid=params, cv=5, scoring = 'accuracy')\n",
    "BCgrid.fit(X,y)\n",
    "plot_clf(plt, BCgrid, X, y); plt.show()\n",
    "report_results(BCgrid) # Show Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** [AdaBoostClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html) **  \n",
    "List of hyperparameters to tune: (base_estimator, n_estimators, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Step 1) Display an example\n",
    "# YOUR CODE HERE\n",
    "\n",
    "clf.fit(X, y)\n",
    "plot_clf(plt, clf, X, y); plt.show()\n",
    "\n",
    "# Step 2) Pick parameters to optimize\n",
    "# YOUR CODE HERE\n",
    "\n",
    "ABgrid = GridSearchCV(estimator=clf, param_grid=params, cv=5, scoring = 'accuracy')\n",
    "ABgrid.fit(X,y)\n",
    "plot_clf(plt, grid, X, y); plt.show()\n",
    "report_results(ABgrid) # Show Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** [GradientBoostingClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html) **  \n",
    "List of hyperparameters to tune: (max_depth, n_estimators, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Step 1) Display an example\n",
    "# YOUR CODE HERE\n",
    "\n",
    "clf.fit(X, y)\n",
    "plot_clf(plt, clf, X, y); plt.show()\n",
    "\n",
    "# Step 2) Pick parameters to optimize\n",
    "# YOUR CODE HERE\n",
    "\n",
    "GBgrid = GridSearchCV(estimator=clf, param_grid=params, cv=5, scoring = 'accuracy')\n",
    "GBgrid.fit(X,y)\n",
    "plot_clf(plt, GBgrid, X, y); plt.show()\n",
    "report_results(GBgrid) # Show Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: Ensemble your ensemble methods\n",
    "Combine your best ensemble classifiers using the `VoteClassifier` approach described in the Introduction.\n",
    "\n",
    "hint: use a copy of the final `grid` variable from each of your optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "clf.fit(X,y)\n",
    "plot_clf(plt, clf, X, y); plt.show()\n",
    "ac = cross_val_score(clf, X, y, cv=5, scoring = 'accuracy')\n",
    "print('mean_test_score = ', ac.mean())\n",
    "print('std_test_score = ', ac.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: Learn hyper-hyper-parameters (Homework)\n",
    "You setup `GridSearchCV` to simultaneously tune some of individual classifier parameters as well explore \"hard\" and \"soft\" voting.  In order to reduce the amount of time it takes to do this, you should restrict the size of the space you search using only a small number values for each parameter. Alternatively you can search to using randomized grid search.\n",
    "\n",
    "hints: \n",
    "1. Use the setup in the Introduction\n",
    "2. Create your set of parameters to optimize by recoding a subset of optimization parameters you used above and also including voting options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Task: Understand Ensemble Method Concepts\n",
    "\n",
    "Below are 6 questions on ensemble methods. Refer the the scikit-learn user guide's section on ensemble methods to help you answer these questions.\n",
    "\n",
    "1. Bagging maintains the variance of the base model while lowering bias. (T/F)\n",
    "2. Predicting with gradient boosted model is slower than predicting with a decision tree. (T/F)\n",
    "3. To make a random forest, you may generate hundreds of trees and then aggregate the results of these tree. Which of the following are true about individual trees in Random Forest? Select all that apply.\n",
    "\n",
    "  (A) Individual trees find best splits on a subset of the features\n",
    "  (B) Individual trees find best splits on all of the features\n",
    "  (C) Individual trees find best splits on a subset of observations\n",
    "  (D) Individual trees find best splits on all of the observations\n",
    "4. Which of the following are true about the “max_depth” hyperparameter in GradientBoostedRegressor and GradientBoostedClassifier? Select all that apply. (A) Lower is better parameter in case of same validation accuracy\n",
    "  (B) Higher is better parameter in case of same validation accuracy\n",
    "  (C) Increase the value of max_depth may overfit the data\n",
    "  (D) Increase the value of max_depth may underfit the data\n",
    "5. For boosting, why is it suggested that the base model be \"weak\"?\n",
    "6. Do bagging and boosting methods always improve model accuracy? When is not an appropriate situation to use ensemble learning methods?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# YOUR ANSWERS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Summary\n",
    "Credit: \n",
    "Some advantages of decision trees are:\n",
    "\n",
    "- Simple to understand and to interpret. Trees can be visualised.\n",
    "- Requires little data preparation. Other techniques often require data normalisation, dummy variables need to be created and blank values to be removed. Note however that this module does not support missing values.\n",
    "- The cost of using the tree (i.e., predicting data) is logarithmic in the number of data points used to train the tree.\n",
    "- Able to handle both numerical and categorical data. Other techniques are usually specialised in analysing datasets that have only one type of variable. See [algorithms](http://scikit-learn.org/stable/modules/tree.html#tree-algorithms) for more information.\n",
    "- Able to handle multi-output problems.\n",
    "- Uses a white box model. If a given situation is observable in a model, the explanation for the condition is easily explained by boolean logic. By contrast, in a black box model (e.g., in an artificial neural network), results may be more difficult to interpret.\n",
    "- Possible to validate a model using statistical tests. That makes it possible to account for the reliability of the model.\n",
    "- Performs well even if its assumptions are somewhat violated by the true model from which the data were generated.\n",
    "\n",
    "The disadvantages of decision trees include:\n",
    "\n",
    "- Decision-tree learners can create over-complex trees that do not generalise the data well. This is called overfitting. Mechanisms such as pruning (not currently supported), setting the minimum number of samples required at a leaf node or setting the maximum depth of the tree are necessary to avoid this problem.\n",
    "- Decision trees can be unstable because small variations in the data might result in a completely different tree being generated. This problem is mitigated by using decision trees within an ensemble.\n",
    "- The problem of learning an optimal decision tree is known to be NP-complete under several aspects of optimality and even for simple concepts. Consequently, practical decision-tree learning algorithms are based on heuristic algorithms such as the greedy algorithm where locally optimal decisions are made at each node. Such algorithms cannot guarantee to return the globally optimal decision tree. This can be mitigated by training multiple trees in an ensemble learner, where the features and samples are randomly sampled with replacement.\n",
    "- There are concepts that are hard to learn because decision trees do not express them easily, such as XOR, parity or multiplexer problems.\n",
    "- Decision tree learners create biased trees if some classes dominate. It is therefore recommended to balance the dataset prior to fitting with the decision tree.\n",
    "\n",
    "Credit: Much of the content and support code from this lab is based on material from scikit-learn's wonderful on-line documentation. Reading through it's tutorial and introductory material while playing with the examples provided, and creating your own is a great way to learn more about Machine Learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
