{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 6b: Multiple models & Ensemble Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "- Introduction\n",
    " - Voting Methods \n",
    "- Ensemble Learning \n",
    "    - Bagging\n",
    "        - Random Forest\n",
    "    - Boosting\n",
    "        - AdaBoost\n",
    "        - GradientBoost\n",
    "- Summary\n",
    "\n",
    "### What's missing?\n",
    "This lab provides a helpful introduction to the use of Multiple Models and Ensemble Methods for classification.  There are two topics it doesn't cover that should be addressed when using these approaches in the world.\n",
    "1. Stratification: When the number of class labels is unequal in your data, or the data is not iid it is important to use stratified sampling to ensure the number of classes in the test and training sets closely match those found in the original data.  The sklearn parameter `stratify` is often available to help one do this.\n",
    "2. Several of the methods considered here construct classifiers via random sampling or other random selections.  In sklearn, setting the `random_state` parameter to a particular seed when using these methods will ensure you generate reproducible results between runs.  This can be helpful when debugging or trying to understand the big picture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In most areas, having multiple experts work on a problem often leads to a better solution.  The same idea can be applied to Machine Learning,  where the experts are different _estimators_ (e.g., classifiers, regressors, neural nets).  As in the real world, if the estimators are generally accurate but \"have different perspectives\", their combination will perform better than any single one of them.\n",
    "\n",
    "In the Introduction section of this lab, we will use two features of the iris dataset (sepal length (cm), petal length (cm)) to classify iris data.   The code below shows the outcome of combining the results of a three different classifiers for this task. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Loading some example data\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, [0, 2]]  # sepal length (cm), petal length (cm)\n",
    "y = iris.target\n",
    "\n",
    "# Training classifiers\n",
    "clf1 = DecisionTreeClassifier(max_depth=3)\n",
    "clf2 = KNeighborsClassifier(n_neighbors=10)\n",
    "clf3 = SVC(kernel='rbf', probability=True)\n",
    "eclf = VotingClassifier(estimators=[('dt', clf1), ('knn', clf2),\n",
    "                                    ('svc', clf3)],\n",
    "                        voting=\"hard\", weights=[2, 1, 1])\n",
    "\n",
    "clf1.fit(X, y)\n",
    "clf2.fit(X, y)\n",
    "clf3.fit(X, y)\n",
    "eclf.fit(X, y)\n",
    "\n",
    "# Plotting decision regions\n",
    "def plot_clf(plt, clf, X, y):\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
    "                     np.arange(y_min, y_max, 0.1))\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, alpha=0.4)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=20, edgecolor='k')\n",
    "\n",
    "f, axarr = plt.subplots(2, 2, sharex='col', sharey='row', figsize=(10, 8))\n",
    "for idx, clf, tt in zip(product([0, 1], [0, 1]),\n",
    "                        [clf1, clf2, clf3, eclf],\n",
    "                        ['Decision Tree (depth=4)', 'kNN (k=10)',\n",
    "                         'SVC(kernel=rbf)', 'Combined (%s voting)'%(eclf.voting)]):\n",
    "    plot_clf(axarr[idx[0], idx[1]], clf,X,y)\n",
    "    axarr[idx[0], idx[1]].set_title(tt)\n",
    "    # Compute and display out of fold accuracy\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "ac = cross_val_score(eclf, X, y, cv=5, scoring = 'accuracy')\n",
    "print(\"Combined (%s voting) classifier accuracy:\"%(eclf.voting))\n",
    "print(\" mean_test_score +/ std_test_score\\n %0.3f +/- %0.2f\"%(ac.mean(), ac.std()/2.0))\n",
    "print(iris.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Estimators / Voting\n",
    "In the above example, *voting* is used to determine the output of the combined classifier.\n",
    "\n",
    "In majority *voting*, the predicted class label for a particular sample is the class label that represents the majority (mode) of the labels predicted by each individual classifier.\n",
    "E.g., if the predictions for a given sample are\n",
    "* classifier 1 -> label A\n",
    "* classifier 2 -> label A\n",
    "* classifier 3 -> label B  \n",
    "the combined prediction is “label A”.\n",
    "\n",
    "Weights can also be applied to make some classifier's votes count more than others. \n",
    "\n",
    "Many classifiers, when given a set of features, also generate estimates of the probability of each possible label being present.  These probabilities can be combined to form more nuanced predictions.  \n",
    "\n",
    "One way to do this is via _soft voting_.  With this approach, the weighted sum $S(k)$ of probabilities associated with each possible class label is computed, and then the label associated with the largest sum is selected.  For example, in three classifier case, using weights $w_1, w_2$ and $w_3$.\n",
    "\n",
    "$$S(k) = w_1 P_1(k)+w_2 P_2(k) + w_3 P_3(k)$$\n",
    "\n",
    "The final label (soft vote) for the data corresponds to the one with largest sum.\n",
    "\n",
    "$$\\text{soft vote} = \\text{argmax } S(k)$$\n",
    "\n",
    "Example:  The table below contains label probabilities for a 3-class classification problem with class labels A,B,C.\n",
    "\n",
    " classifier i | P_i(A)       | P_i(B)       | P_i(C) \n",
    " -------------|:------------:|:------------:|:-----------:  \n",
    " classifier 1 |  0.1  | 0.8 | 0.1\n",
    " classifier 2 |  0.5  | 0.4 | 0.1\n",
    " classifier 3 |  0.6  | 0.3 | 0.1\n",
    "  \n",
    "\n",
    "Using weights of 1/3 on all classifiers (i.e., $w_1=1/3, w_2=1/3, w_3=1/3$, $S(K)$ is found by applying the weights and summing each column.  ans has the following values:\n",
    "\n",
    "  | S(A) | S(B) | S(C)\n",
    " -----|------|------| ---\n",
    " S(k) |  .4 | .5  | 0.1\n",
    " \n",
    "\n",
    "The predicted class label (soft vote) is for class label B, since it generates the highest weight sum.  Below is a bar chart showing the values of $P_i(k)$ and $S(k)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame({'$P_1(k)$': [.1,.8,.1], '$P_3(k)$': [.5,.4,.1],\n",
    "                 '$P_2(k)$': [.6,.3,.1], '$S(k)$':[.4, .5, .1]}, \n",
    "             index = ['A', 'B', 'C'])\n",
    "df.plot.bar(grid='on'); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: Hard and Soft Voting\n",
    "1. In general, when using soft max voting with n classifiers, if one uses a uniform vote weighting of 1/n, do the soft voting scores define a probability measure on possible class labels?\n",
    "\n",
    "2. Run in isolation, what label would be selected as the output for each classifier in the table and plot above?  \n",
    "Using a majority voting (hard voting), what class label would be selected using these outputs?  \n",
    "Do soft **soft voting** and **hard voting** always agree?  Always disagree?\n",
    "\n",
    "3. The [sklearn.ensemble.VotingClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html) \n",
    "allows one to perform both majority (hard) voting and soft vote class with a collection of individual classifiers.\n",
    "The `voting` parameter selects the method to use. Re-run the code above by changing the `voting` parameter from `\"hard\"` to `\"soft\"`.  \n",
    "Looking at the decision boundaries, does it look like the combined classifier results improve? What does the out of fold accuracy show? Explain your reasoning.\n",
    "\n",
    "Put your answers below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BEGIN SOLUTION\n",
    "1) **In general, when using soft max voting with n classifiers, if one uses a uniform vote weighting of 1/n, do the soft voting scores define a probability measure on possible class labels?**\n",
    "Yes, in fact anytime the weights $w_k$ define a probability distribution (i.e., are non-negative and sum to one), the resulting label soft vote scores will define a label probability distribution.  To show this you sum S(k) over all labels, and show that it equal one, i.e.,\n",
    "\n",
    "$$\\Sigma_k S(k) = \\Sigma_k \\Sigma_i w_i P_i(k) = \\Sigma_i \\Sigma_k w_i P_i(k) = \\Sigma_i w_i \\Sigma_k P_i(k) =  \\Sigma_k w_k = 1$$\n",
    "\n",
    "2) **What label would be selected as the output for each classifier in the table above?**\n",
    "\n",
    "classifier i | Most likely class label\n",
    "-------------|:-: \n",
    "classifier 1 | B  \n",
    "classifier 2 | A \n",
    "classifier 3 | A  \n",
    "\n",
    "**Using a majority voting, what class label would be selected using these outputs?**  \n",
    "A would be selected using majority (hard) voting.\n",
    "\n",
    "**What do you conclude about the agreement between **soft voting** and **hard voting**?**\n",
    "Hard and soft voting may or may not agree.\n",
    "\n",
    "3) **Looking at the decision boundaries, does it look like the combined classifier results improve? What does the out of fold accuracy show?**\n",
    "The Combined (soft voting) classifier decision boundaries look like they handle outliers better than the hard voting case.  It also has higher accuracy.  \n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameter optimization \n",
    "Independently optimizing each estimator before combining it with others is a good first step, but given it's their combined performance that is of interest, simultaneously optimizing all hyper-parameters may yield additional improvements.  \n",
    "\n",
    "Below is an simultaneous optimization using sklearn.GridSearchCV, in the example, voting strategies for the combination classifier are also explored.\n",
    "\n",
    "More detailed examples involving full _pipeline optimization_, including feature selection can be found in this [MLxtend] article](http://rasbt.github.io/mlxtend/user_guide/classifier/EnsembleVoteClassifier/) on Sebastian Raschka's `EnsembleVoteClassifier` (which became the basis for the `scikit-learn.VotingClassifier`).  See the pipeline and feature documentation at sklearn as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "clf1 = DecisionTreeClassifier()\n",
    "clf2 = KNeighborsClassifier()\n",
    "clf3 = SVC(kernel='rbf', probability=True)\n",
    "eclf = VotingClassifier(estimators=[('DT', clf1), ('kNN', clf2), ('SVC', clf3)])\n",
    "\n",
    "params = {'DT__max_depth': [3, 4, 5],\n",
    "          'kNN__n_neighbors': [6, 7, 8],\n",
    "          'voting': ['hard','soft']}\n",
    "          \n",
    "grid = GridSearchCV(estimator=eclf, param_grid=params, cv=5, scoring = 'accuracy')\n",
    "\n",
    "X = iris.data[:, [0, 2]]\n",
    "y = iris.target\n",
    "grid.fit(X, y)\n",
    "\n",
    "def report_results(grid):\n",
    "    cv_keys = ('mean_test_score', 'std_test_score', 'params')\n",
    "    print('mean_test_score +/ std_test_score, {params}') \n",
    "    for r, _ in enumerate(grid.cv_results_['mean_test_score']):\n",
    "        bf = '*' if grid.cv_results_[cv_keys[0]][r]==grid.best_score_ else ' '\n",
    "        print(bf+\"%0.3f +/- %0.2f %r\"\n",
    "          % (grid.cv_results_[cv_keys[0]][r],\n",
    "             grid.cv_results_[cv_keys[1]][r] / 2.0,\n",
    "             grid.cv_results_[cv_keys[2]][r]))\n",
    "\n",
    "report_results(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: Working with GridSearchCV\n",
    "Using the code block below, determine the best estimator found by `GridSearchCV` and then do the following:\n",
    "1. Print out the best model's average accuracy score on out of fold data. \n",
    "2. Print out the best model's parameters.\n",
    "3. Use `plot_clf` to display its decision boundaries.\n",
    "\n",
    "hint: This information is available in `grid` variable that was computed in the previous cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "best_estimator = grid.best_estimator_\n",
    "\n",
    "print('Best of of fold accuracy =', grid.best_score_)\n",
    "print('Best parameters =',  grid.best_params_)\n",
    "\n",
    "plot_clf(plt, grid, X, y)\n",
    "plt.show()\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Methods\n",
    "The goal of **ensemble methods** is to _automatically create_ a set of \n",
    "base estimators (using the method's learning algorithm) which are then combined \n",
    "using a voting approach.  \n",
    "\n",
    "Two families of ensemble methods are usually distinguished:\n",
    "- In **averaging methods**, the driving principle is to build several estimators \n",
    "independently and then to combine their predictions using using averaging, weighted averaging, or voting. Typically, the combined \n",
    "estimator is usually better than any single base estimator because \n",
    "its variance is reduced. **Examples:** \n",
    "  - [Bagging methods](http://scikit-learn.org/stable/modules/ensemble.html#bagging), \n",
    "  - [Forests of randomized trees](http://scikit-learn.org/stable/modules/ensemble.html#forest)  \n",
    "\n",
    "\n",
    "- By contrast, in **boosting methods**, base estimators are built sequentially \n",
    "in order to try to reduce the bias of previously constructed estimators.\n",
    "Here again, the motivation is to combine several weak models to produce a more powerful one. **Examples:** \n",
    "  - [AdaBoost](http://scikit-learn.org/stable/modules/ensemble.html#adaboost)\n",
    "  - [Gradient Tree Boosting](http://scikit-learn.org/stable/modules/ensemble.html#gradient-boosting), … "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging\n",
    "\n",
    "Bagging another name for ensemble averaging.  As stated earlier, the goal is to improve the performance of simple models and reduce overfitting of more complex models. With bagging, several models are fitted on different samples of the population (taken with replacement). Then, these models are aggregated by using their average, weighted average or a voting system.\n",
    "\n",
    "A key insight for bagging is that by averaging (or generally aggregating) many low bias, high variance models, we can reduce the variance while retaining the low bias. Here’s an example of this for density estimation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://qph.ec.quoracdn.net/main-qimg-55c44d63831742ddd387541a428fcedf.webp\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each estimate is centered around the true density, but is overly complicated (low bias, high variance). By averaging them out, we get a smoothed version of them (low variance), still centered around the true density (low bias). (Jonathan Gordon on Quora)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest\n",
    "\n",
    "Random forest is a variant of bagging which results in a more random but potentially more powerful classifier.  \n",
    "\n",
    "Random Forests use subsets of the data (as in regular bagging) to create a set of decision tree classifiers, where a certain amount of additional randomness is introduced into the fitting method for each tree.  When splitting a node during the construction of a tree, the split that is chosen is no longer the best split among all features. Rather, the split picked is the best split among a random subset of the features. As a result of this randomness, the bias of the forest usually slightly increases (with respect to the bias of a single non-random tree) but, due to averaging, its variance decreases, usually more than compensating for the increase in bias, hence yielding an overall better model.\n",
    "\n",
    "This additional randomness is used to try and make the underlying trees more independent, which improves performance.\n",
    "\n",
    "\n",
    "Below is an example setup call to create a random forest classifier in sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier \n",
    "# max_features=\"sqrt\" => *max_features=sqrt(n_features)\n",
    "clf = RandomForestClassifier(n_estimators=20, max_features='sqrt') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameter **n_estimators** controls the number of trees in the forest and the parameter **max_features** controls the number of randomly selected features to consider when looking for the best split. \n",
    "\n",
    "Below are some snippets from the [RandomForestClassifier]http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier) documentation:\n",
    "\n",
    "**n_estimators** : integer, optional (default=10)\n",
    "\n",
    "> The number of trees in the forest.\n",
    "\n",
    "**max_features** : int, float, string or None, optional (default=\"auto\")\n",
    "\n",
    "> The number of features to consider when looking for the best split:\n",
    ">\n",
    "> -   If int, then consider *max_features* features at each split.\n",
    "> -   If float, then *max_features* is a percentage and*int(max_features * n_features)* features are considered at each split.\n",
    "> -   If \"auto\", then *max_features=sqrt(n_features)*.\n",
    "> -   If \"sqrt\", then *max_features=sqrt(n_features)* (same as \"auto\").\n",
    "> -   If \"log2\", then *max_features=log2(n_features)*.\n",
    "> -   If None, then *max_features=n_features*.\n",
    ">\n",
    "> Note: the search for a split does not stop until at least one valid partition of the node samples is found, even if it requires to effectively inspect more than `max_features` features.\n",
    "\n",
    "Notice that when *max_features* is set to the total number of features in the data, random feature selection during splitting is eliminated, and Random Forest reduces to decision tree based bagging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting \n",
    "\n",
    "Boosting is a general ensemble method that creates a stronger model from a number of weaker models.\n",
    "\n",
    "This is done by building a model from the training data, then creating a second model that attempts to correct the errors from the first model. Models are added until the training set is predicted perfectly or a maximum number of models are added."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adaptive Boosting (AdaBoost)\n",
    "\n",
    "The core principle of AdaBoost is to fit a sequence of weak learners (i.e., models that are at least slightly better than random guessing, such as small decision trees) on weighted versions of the data. As iterations proceed, examples that are difficult to predict receive ever-increasing influence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/boosting1.png\" style=\"width:600px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) We start with one weak leaner (for example a [decision tree stump](https://en.wikipedia.org/wiki/Decision_stump)) to classify training samples.  \n",
    "(2) In the next round, we then train another weak learner (e.g. decision tree stump) that focuses on getting the samples that were misclassified in (1). We achieve this by putting a larger weight on the previously misclassified training samples.  \n",
    "(3) The 2nd classifier will likely get some other samples wrong, so we can re-adjust the weights and train a third classifier accordingly.  \n",
    "(4) Same logic from (3) is applied.\n",
    "\n",
    "\n",
    "In a nutshell, we can summarize “Adaboost” as “adaptive” or “incremental” learning from mistakes. Eventually, we will come up with a model that has a lower bias than an individual decision tree (thus, it is less likely to underfit the training data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting\n",
    "\n",
    "Gradient Boosting is another popular boosting technique that is similar to Adaptive Boosting.  The major difference is that Gradient Boosting identifies and corrects the short comings of weaker learners using gradients in the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scikit-learn Ensemble Methods\n",
    "You will now use the sci-kit to setup and optimize the ensemble classifiers described above. \n",
    "\n",
    "Understanding useful model parameters associated with a these powerful classification approaches takes time and practice, and as with all classification approaches, what works well will be problem domain or even data set specific.  \n",
    "\n",
    "Please run the code below to create and display a random blob data set.  The section of the lab will now focus on classification tasks using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "XX, yy = make_blobs(n_samples=1000, centers=20, random_state=42)\n",
    "yy = yy % 2\n",
    "plt.scatter(XX[:, 0], XX[:, 1], c=yy, s=20, edgecolor='k'); plt.show()\n",
    "yy[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: Ensemble training\n",
    "Read the hyper-linked documentation and build models on the  blob dataset (XX, yy) using the following approaches:\n",
    "\n",
    "1. [BaggingClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html)\n",
    "2. [RandomForestClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)   \n",
    "3. [AdaBoostClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html) \n",
    "4. [GradientBoostingClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)  \n",
    "\n",
    "Each model has parameters you can adjust.  Typically these are general parameters related to the underlying classifiers (e.g. when using decision trees,\n",
    "**max_depth**, **min_samples_leaf**) and other parameters related to how the different base classifiers are generated (e.g., when using RandomForestClassifier, **n_estimators**, **max_features**).\n",
    "\n",
    "Below is a fully worked example using [DecisionTreeClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) on the blob dataset.  The decision boundaries provide insight into how the classifier is working, but out of fold accuracy scores are what's most important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[DecisionTreeClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier \n",
    "\n",
    "# Step 1) Display an example\n",
    "clf = DecisionTreeClassifier(max_depth=4)\n",
    "\n",
    "clf.fit(XX, yy)\n",
    "plot_clf(plt, clf, XX, yy)\n",
    "plt.show()\n",
    "\n",
    "# Step 2) Pick parameters to optimize\n",
    "params = {'max_depth': [2, 4, 8, 16], 'min_samples_leaf':[2, 4, 8]}\n",
    "\n",
    "DTgrid = GridSearchCV(estimator=clf, param_grid=params, cv=5, scoring = 'accuracy')\n",
    "DTgrid.fit(XX,yy)\n",
    "plot_clf(plt, DTgrid.best_estimator_, XX, yy); plt.show()\n",
    "report_results(DTgrid) # Show Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[BaggingClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html)** \n",
    "\n",
    "hint: try `base_estimator=DTgrid.best_estimator_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.base import clone\n",
    "\n",
    "# Step 1) Display an example\n",
    "### BEGIN SOLUTION\n",
    "clf = BaggingClassifier(base_estimator=DTgrid.best_estimator_, n_estimators=4, random_state=42)\n",
    "### END SOLUTION\n",
    "\n",
    "clf.fit(XX, yy)\n",
    "plot_clf(plt, clf, XX, yy); plt.show()\n",
    "\n",
    "# Step 2) Pick parameters to optimize\n",
    "### BEGIN SOLUTION\n",
    "params = {'n_estimators': [2, 4, 8, 16]}\n",
    "### END SOLUTION\n",
    "\n",
    "BCgrid = GridSearchCV(estimator=clf, param_grid=params, cv=5, scoring = 'accuracy')\n",
    "BCgrid.fit(XX,yy)\n",
    "plot_clf(plt, BCgrid.best_estimator_, XX, yy); plt.show()\n",
    "report_results(BCgrid) # Show Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[RandomForestClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier \n",
    "\n",
    "# Step 1) Display an example\n",
    "### BEGIN SOLUTION\n",
    "clf = RandomForestClassifier(max_depth=4, random_state=42)\n",
    "### END SOLUTION\n",
    "\n",
    "clf.fit(XX, yy)\n",
    "plot_clf(plt, clf, XX, yy); plt.show()\n",
    "\n",
    "# Step 2) Pick parameters to optimize\n",
    "### BEGIN SOLUTION\n",
    "params = {'max_depth': [8, 16, 32], 'min_samples_leaf':[2, 4, 8]}\n",
    "### END SOLUTION\n",
    "\n",
    "RFgrid = GridSearchCV(estimator=clf, param_grid=params, cv=5, scoring = 'accuracy')\n",
    "RFgrid.fit(XX,yy)\n",
    "plot_clf(plt, RFgrid.best_estimator_, XX, yy); plt.show()\n",
    "report_results(RFgrid) # Show Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[AdaBoostClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html)** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Step 1) Display an example\n",
    "### BEGIN SOLUTION\n",
    "clf = AdaBoostClassifier(base_estimator=RFgrid.best_estimator_, n_estimators=4, random_state = 42)\n",
    "### END SOLUTION\n",
    "\n",
    "clf.fit(XX, yy)\n",
    "plot_clf(plt, clf, XX, yy); plt.show()\n",
    "\n",
    "# Step 2) Pick parameters to optimize\n",
    "### BEGIN SOLUTION\n",
    "params = {'n_estimators':[2, 3, 4, 5, 8, 16, 32]}\n",
    "### END SOLUTION\n",
    "\n",
    "ABgrid = GridSearchCV(estimator=clf, param_grid=params, cv=5, scoring = 'accuracy')\n",
    "ABgrid.fit(XX, yy)\n",
    "plot_clf(plt, ABgrid.best_estimator_, XX, yy); plt.show()\n",
    "report_results(ABgrid) # Show Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[GradientBoostingClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)**   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Step 1) Display an example\n",
    "### BEGIN SOLUTION\n",
    "clf = GradientBoostingClassifier(max_depth=4, random_state = 42)\n",
    "### END SOLUTION\n",
    "\n",
    "clf.fit(XX, yy)\n",
    "plot_clf(plt, clf, XX, yy); plt.show()\n",
    "\n",
    "# Step 2) Pick parameters to optimize\n",
    "### BEGIN SOLUTION\n",
    "params = {'max_depth': [2, 4, 8, 16], 'min_samples_leaf':[2, 4, 8]}\n",
    "### END SOLUTION\n",
    "\n",
    "GBgrid = GridSearchCV(clf, param_grid=params, cv=5, scoring = 'accuracy')\n",
    "GBgrid.fit(XX,yy)\n",
    "plot_clf(plt, GBgrid.best_estimator_, XX, yy); plt.show()\n",
    "report_results(GBgrid) # Show Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: Ensemble your ensemble methods\n",
    "Combine your best ensemble classifiers using the `VoteClassifier` approach described in the Introduction.\n",
    "\n",
    "hint: use the `.best_estimator_` attribute from each of your grid variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "clf = VotingClassifier(estimators=[('RF', RFgrid.best_estimator_), \n",
    "                                   ('BC', BCgrid.best_estimator_), \n",
    "                                   ('AB', ABgrid.best_estimator_), \n",
    "                                   ('GB', GBgrid.best_estimator_)],\n",
    "                        voting='hard')\n",
    "\n",
    "### END SOLUTION\n",
    "clf.fit(XX,yy)\n",
    "plot_clf(plt, clf, XX, yy); plt.show()\n",
    "ac = cross_val_score(clf, XX, yy, cv=5, scoring = 'accuracy')\n",
    "print(\"Combination (%s voting) classifier accuracy voting:\"%(clf.voting))\n",
    "print(\" mean_test_score +/ std_test_score\\n %0.3f +/- %0.2f\"%(ac.mean(), ac.std()/2.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: Learn hyper-hyper-parameters (Homework)\n",
    "Setup `GridSearchCV` to simultaneously tune some of individual classifier parameters as well explore \"hard\" and \"soft\" voting.  In order to reduce the amount of time it takes to do this, you should restrict the size of the space you search using only a small number values for each parameter. Alternatively you can search to using randomized grid search via `RandomizedSearchCV`.\n",
    "\n",
    "hints: \n",
    "1. Use the setup in the Introduction\n",
    "2. Create your set of parameters to optimize by recoding a subset of optimization parameters you used above and also including voting options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "clf = VotingClassifier(estimators=[('BC', BCgrid.best_estimator_),\n",
    "                                   ('RF', RFgrid.best_estimator_),  \n",
    "                                   ('AB', ABgrid.best_estimator_), \n",
    "                                   ('GB', GBgrid.best_estimator_)],\n",
    "                        voting='soft')\n",
    "\n",
    "params = {\n",
    "  'BC__n_estimators': [1, 2, 3],\n",
    "  'RF__max_depth': [7, 8, 9], 'RF__min_samples_leaf':[3, 4, 5],\n",
    "  'AB__n_estimators':[3,4,5],\n",
    "  'GB__max_depth': [3, 4, 5], 'GB__min_samples_leaf':[3, 4, 5],\n",
    "  'voting':['hard','soft']}\n",
    "\n",
    "#ALLgrid = GridSearchCV(estimator=clf, param_grid=params, cv=5, \n",
    "#                       scoring = 'accuracy', verbose=1)\n",
    "ALLgrid = RandomizedSearchCV(estimator=clf, param_distributions=params, cv=5, \n",
    "                             scoring = 'accuracy',\n",
    "                             random_state=1, verbose=1, n_iter=30)\n",
    "print('Starting ALLgrid optimization:')\n",
    "ALLgrid.fit(XX,yy)\n",
    "plot_clf(plt, ALLgrid.best_estimator_, XX, yy); plt.show()\n",
    "print('Best classifier found: ALLGrid(%s)'%(ALLgrid.best_params_))\n",
    "print(' Out of fold accuracy = %f'%(ALLgrid.best_score_))\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Task: Understand Ensemble Method Concepts\n",
    "\n",
    "Below are 6 questions on ensemble methods. Refer the the scikit-learn user guide's section on ensemble methods to help you answer these questions.\n",
    "\n",
    "1. Bagging maintains the variance of the base model while lowering bias. (T/F)\n",
    "2. Predicting with gradient boosted model is slower than predicting with a decision tree. (T/F)\n",
    "3. To make a random forest, you may generate hundreds of trees and then aggregate the results of these tree. Which of the following are true about individual trees in Random Forest? Select all that apply.\n",
    "\n",
    "  (A) Individual trees find best splits on a subset of the features\n",
    "  (B) Individual trees find best splits on all of the features\n",
    "  (C) Individual trees find best splits on a subset of observations\n",
    "  (D) Individual trees find best splits on all of the observations\n",
    "4. Which of the following are true about the “max_depth” hyperparameter in GradientBoostedRegressor and GradientBoostedClassifier? Select all that apply. (A) Lower is better parameter in case of same validation accuracy\n",
    "  (B) Higher is better parameter in case of same validation accuracy\n",
    "  (C) Increase the value of max_depth may overfit the data\n",
    "  (D) Increase the value of max_depth may underfit the data\n",
    "5. For boosting, why is it suggested that the base model be \"weak\"?\n",
    "6. Do bagging and boosting methods always improve model accuracy? When is not an appropriate situation to use ensemble learning methods?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### BEGIN SOLUTION\n",
    "1. True\n",
    "2. True.Boosting requires aggregating the results of multiple models.\n",
    "3. (A) and (C). Random forest is based on bagging concept, that consider faction of sample and faction of feature for building the individual trees.\n",
    "4. (A) and (C). Increasing the depth may overfit the data and if the validation accuracies are same we always prefer the small depth in final model building.\n",
    "5. To prevent overfitting, since the complexity of the overall learner increases at each step. Starting with weak learners implies the final classifier will be less likely to overfit.\n",
    "6. The main disadvantages of ensemble algorithms are the lack of interpretation, learning time and memory constraints, difficulty in measuring correlation between classifiers from different types of learners.\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Summary\n",
    "Some advantages of decision trees are:\n",
    "\n",
    "- Simple to understand and to interpret. Trees can be visualised.\n",
    "- Requires little data preparation. Other techniques often require data normalisation, dummy variables need to be created and blank values to be removed. Note however that this module does not support missing values.\n",
    "- The cost of using the tree (i.e., predicting data) is logarithmic in the number of data points used to train the tree.\n",
    "- Able to handle both numerical and categorical data. Other techniques are usually specialised in analysing datasets that have only one type of variable. See [algorithms](http://scikit-learn.org/stable/modules/tree.html#tree-algorithms) for more information.\n",
    "- Able to handle multi-output problems.\n",
    "- Uses a white box model. If a given situation is observable in a model, the explanation for the condition is easily explained by boolean logic. By contrast, in a black box model (e.g., in an artificial neural network), results may be more difficult to interpret.\n",
    "- Possible to validate a model using statistical tests. That makes it possible to account for the reliability of the model.\n",
    "- Performs well even if its assumptions are somewhat violated by the true model from which the data were generated.\n",
    "\n",
    "The disadvantages of decision trees include:\n",
    "\n",
    "- Decision-tree learners can create over-complex trees that do not generalise the data well. This is called overfitting. Mechanisms such as pruning (not currently supported), setting the minimum number of samples required at a leaf node or setting the maximum depth of the tree are necessary to avoid this problem.\n",
    "- Decision trees can be unstable because small variations in the data might result in a completely different tree being generated. This problem is mitigated by using decision trees within an ensemble.\n",
    "- The problem of learning an optimal decision tree is known to be NP-complete under several aspects of optimality and even for simple concepts. Consequently, practical decision-tree learning algorithms are based on heuristic algorithms such as the greedy algorithm where locally optimal decisions are made at each node. Such algorithms cannot guarantee to return the globally optimal decision tree. This can be mitigated by training multiple trees in an ensemble learner, where the features and samples are randomly sampled with replacement.\n",
    "- There are concepts that are hard to learn because decision trees do not express them easily, such as XOR, parity or multiplexer problems.\n",
    "- Decision tree learners create biased trees if some classes dominate. It is therefore recommended to balance the dataset prior to fitting with the decision tree.\n",
    "\n",
    "**Full Credit:** Much of the content and support code from this lab is based on material from scikit-learn's wonderful [on-line documentation](http://scikit-learn.org/stable/documentation.html). Reading through its tutorials, introductory material and package pages, while playing with the examples provided, and creating your own, is a great way to learn more about Machine Learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
